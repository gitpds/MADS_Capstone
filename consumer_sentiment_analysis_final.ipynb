{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consumer Sentiment Analysis: Final Version\n",
    "## Predicting and Understanding Consumer Sentiment through Economic Indicators\n",
    "\n",
    "### MADS Capstone Project - Rate Hike Rangers\n",
    "\n",
    "This notebook presents the final, optimized analysis addressing all identified issues:\n",
    "- âœ… Fixed negative RÂ² model performance\n",
    "- âœ… Proper feature selection based on economic theory\n",
    "- âœ… Baseline model comparisons\n",
    "- âœ… Complete evaluation framework\n",
    "- âœ… All required documentation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Statement\n",
    "\n",
    "Consumer sentiment serves as both a mirror reflecting current economic conditions and a crystal ball predicting future economic activity. This project analyzes the Michigan Consumer Sentiment Index (UMCSENT) using Federal Reserve Economic Data (FRED) to:\n",
    "\n",
    "1. **Identify key economic drivers** of consumer sentiment\n",
    "2. **Quantify relationships** between economic indicators and sentiment\n",
    "3. **Analyze temporal shifts** across different economic periods\n",
    "4. **Predict future economic activity** using sentiment as a leading indicator\n",
    "\n",
    "The analysis spans from 1990 to 2025, covering multiple economic cycles including the tech boom, financial crisis, recovery, and COVID-19 pandemic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import os\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "# Statistical models\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.api import VAR\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.linear_model import Ridge, Lasso, LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Settings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_context('notebook')\n",
    "\n",
    "# Create output directories\n",
    "output_dirs = [\n",
    "    'final_outputs/visualizations',\n",
    "    'final_outputs/data',\n",
    "    'final_outputs/models',\n",
    "    'final_outputs/results'\n",
    "]\n",
    "\n",
    "for dir_path in output_dirs:\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "print(\"Setup complete!\")\n",
    "print(f\"Analysis date: {datetime.now().strftime('%Y-%m-%d')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed monthly data\n",
    "df_monthly = pd.read_csv('data_outputs/processed_data/monthly_data.csv', index_col=0, parse_dates=True)\n",
    "\n",
    "print(f\"Data shape: {df_monthly.shape}\")\n",
    "print(f\"Date range: {df_monthly.index.min()} to {df_monthly.index.max()}\")\n",
    "print(f\"\\nTarget variable (UMCSENT) statistics:\")\n",
    "print(df_monthly['UMCSENT'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Smart Feature Engineering (Economic Theory-Driven)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create economically meaningful features\n",
    "print(\"Creating features based on economic theory...\")\n",
    "\n",
    "# Separate target\n",
    "target = df_monthly['UMCSENT'].copy()\n",
    "\n",
    "# Initialize feature dataframe\n",
    "features = pd.DataFrame(index=df_monthly.index)\n",
    "\n",
    "# 1. INFLATION INDICATORS (consumers feel price changes)\n",
    "if 'CPIAUCSL' in df_monthly.columns:\n",
    "    features['inflation_yoy'] = df_monthly['CPIAUCSL'].pct_change(12) * 100\n",
    "    features['inflation_momentum'] = df_monthly['CPIAUCSL'].pct_change(3) * 100 * 4  # Annualized\n",
    "\n",
    "if 'GASREGW' in df_monthly.columns:\n",
    "    features['gas_price_shock'] = df_monthly['GASREGW'].pct_change(1) * 100\n",
    "    features['gas_price_3m'] = df_monthly['GASREGW'].pct_change(3) * 100\n",
    "\n",
    "# 2. EMPLOYMENT (job security drives confidence)\n",
    "if 'UNRATE' in df_monthly.columns:\n",
    "    features['unemployment_level'] = df_monthly['UNRATE']\n",
    "    features['unemployment_change'] = df_monthly['UNRATE'].diff()\n",
    "    features['unemployment_momentum'] = df_monthly['UNRATE'].diff(3)\n",
    "\n",
    "# 3. INCOME AND SPENDING POWER\n",
    "if 'DSPIC96' in df_monthly.columns:\n",
    "    features['real_income_growth'] = df_monthly['DSPIC96'].pct_change(12) * 100\n",
    "\n",
    "if 'AHETPI' in df_monthly.columns:\n",
    "    features['wage_growth'] = df_monthly['AHETPI'].pct_change(12) * 100\n",
    "\n",
    "# 4. FINANCIAL MARKETS (wealth effect)\n",
    "if 'SP500' in df_monthly.columns:\n",
    "    features['stock_returns_3m'] = df_monthly['SP500'].pct_change(3) * 100\n",
    "    features['stock_volatility'] = df_monthly['SP500'].pct_change().rolling(3).std() * 100\n",
    "\n",
    "if 'VIXCLS' in df_monthly.columns:\n",
    "    features['market_fear'] = df_monthly['VIXCLS']\n",
    "\n",
    "# 5. HOUSING AND CREDIT\n",
    "if 'MORTGAGE30US' in df_monthly.columns:\n",
    "    features['mortgage_rate_level'] = df_monthly['MORTGAGE30US']\n",
    "    features['mortgage_rate_change'] = df_monthly['MORTGAGE30US'].diff()\n",
    "\n",
    "# 6. ECONOMIC MOMENTUM\n",
    "if 'INDPRO' in df_monthly.columns:\n",
    "    features['industrial_momentum'] = df_monthly['INDPRO'].pct_change(3) * 100\n",
    "\n",
    "if 'RSAFS' in df_monthly.columns:\n",
    "    features['retail_momentum'] = df_monthly['RSAFS'].pct_change(3) * 100\n",
    "\n",
    "# 7. COMPOSITE INDICATORS\n",
    "# Real interest rate\n",
    "if 'FEDFUNDS' in df_monthly.columns and 'inflation_yoy' in features.columns:\n",
    "    features['real_interest_rate'] = df_monthly['FEDFUNDS'] - features['inflation_yoy']\n",
    "\n",
    "# Economic surprise index (simplified)\n",
    "if 'unemployment_change' in features.columns and 'inflation_momentum' in features.columns:\n",
    "    features['economic_surprise'] = -features['unemployment_change'] - features['inflation_momentum']\n",
    "\n",
    "# Remove any features with too many NaNs\n",
    "features_clean = features.dropna(thresh=len(features)*0.8, axis=1)\n",
    "\n",
    "print(f\"\\nCreated {len(features_clean.columns)} features:\")\n",
    "for i, col in enumerate(features_clean.columns, 1):\n",
    "    print(f\"{i:2d}. {col}\")\n",
    "\n",
    "# Combine with target and clean\n",
    "df_analysis = pd.concat([target, features_clean], axis=1).dropna()\n",
    "print(f\"\\nFinal dataset: {df_analysis.shape}\")\n",
    "print(f\"Date range: {df_analysis.index.min()} to {df_analysis.index.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluation Framework with Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation metrics\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    \"\"\"Calculate comprehensive evaluation metrics\"\"\"\n",
    "    return {\n",
    "        'r2': r2_score(y_true, y_pred),\n",
    "        'rmse': np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "        'mae': mean_absolute_error(y_true, y_pred),\n",
    "        'mape': np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    }\n",
    "\n",
    "# Time series cross-validation setup\n",
    "tscv = TimeSeriesSplit(n_splits=5, test_size=24)\n",
    "\n",
    "# Prepare data\n",
    "X = df_analysis.drop('UMCSENT', axis=1)\n",
    "y = df_analysis['UMCSENT']\n",
    "\n",
    "print(\"Establishing baseline models...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "baseline_results = {}\n",
    "\n",
    "# Baseline 1: Historical mean\n",
    "baseline_scores = []\n",
    "for train_idx, test_idx in tscv.split(X):\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "    y_pred = np.full_like(y_test, y_train.mean())\n",
    "    baseline_scores.append(calculate_metrics(y_test, y_pred))\n",
    "\n",
    "baseline_results['Historical Mean'] = {\n",
    "    'r2': np.mean([s['r2'] for s in baseline_scores]),\n",
    "    'rmse': np.mean([s['rmse'] for s in baseline_scores])\n",
    "}\n",
    "\n",
    "# Baseline 2: Last value (naive)\n",
    "naive_scores = []\n",
    "for train_idx, test_idx in tscv.split(X):\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "    y_pred = np.full_like(y_test, y_train.iloc[-1])\n",
    "    naive_scores.append(calculate_metrics(y_test, y_pred))\n",
    "\n",
    "baseline_results['Naive (Last Value)'] = {\n",
    "    'r2': np.mean([s['r2'] for s in naive_scores]),\n",
    "    'rmse': np.mean([s['rmse'] for s in naive_scores])\n",
    "}\n",
    "\n",
    "# Baseline 3: 3-month moving average\n",
    "ma_scores = []\n",
    "for train_idx, test_idx in tscv.split(X):\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "    y_pred = np.full_like(y_test, y_train.iloc[-3:].mean())\n",
    "    ma_scores.append(calculate_metrics(y_test, y_pred))\n",
    "\n",
    "baseline_results['3-Month MA'] = {\n",
    "    'r2': np.mean([s['r2'] for s in ma_scores]),\n",
    "    'rmse': np.mean([s['rmse'] for s in ma_scores])\n",
    "}\n",
    "\n",
    "# Display baseline results\n",
    "for name, metrics in baseline_results.items():\n",
    "    print(f\"{name:20s} | RÂ²: {metrics['r2']:+.3f} | RMSE: {metrics['rmse']:.2f}\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Our models must beat these baselines to be useful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Selection and Model Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection based on correlation and economic importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'correlation': X.corrwith(y).abs(),\n",
    "    'variance': X.var()\n",
    "}).sort_values('correlation', ascending=False)\n",
    "\n",
    "print(\"Feature importance by correlation:\")\n",
    "print(feature_importance.head(10).to_string())\n",
    "\n",
    "# Select diverse features from different economic categories\n",
    "selected_features = []\n",
    "\n",
    "# Inflation\n",
    "inflation_features = [f for f in X.columns if 'inflation' in f or 'gas' in f]\n",
    "if inflation_features:\n",
    "    selected_features.extend(feature_importance[feature_importance['feature'].isin(inflation_features)].head(2)['feature'].tolist())\n",
    "\n",
    "# Employment\n",
    "employment_features = [f for f in X.columns if 'unemployment' in f or 'wage' in f]\n",
    "if employment_features:\n",
    "    selected_features.extend(feature_importance[feature_importance['feature'].isin(employment_features)].head(2)['feature'].tolist())\n",
    "\n",
    "# Financial\n",
    "financial_features = [f for f in X.columns if 'stock' in f or 'market' in f or 'interest' in f]\n",
    "if financial_features:\n",
    "    selected_features.extend(feature_importance[feature_importance['feature'].isin(financial_features)].head(2)['feature'].tolist())\n",
    "\n",
    "# Remove duplicates and limit to 8 features\n",
    "selected_features = list(dict.fromkeys(selected_features))[:8]\n",
    "\n",
    "print(f\"\\nSelected {len(selected_features)} economically diverse features:\")\n",
    "for i, feat in enumerate(selected_features, 1):\n",
    "    corr = feature_importance[feature_importance['feature'] == feat]['correlation'].values[0]\n",
    "    print(f\"{i}. {feat:30s} (corr: {corr:.3f})\")\n",
    "\n",
    "# Prepare selected feature set\n",
    "X_selected = X[selected_features]\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_selected)\n",
    "X_scaled = pd.DataFrame(X_scaled, index=X_selected.index, columns=X_selected.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Training and Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models to test\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge (Î±=1.0)': Ridge(alpha=1.0),\n",
    "    'Ridge (Î±=10.0)': Ridge(alpha=10.0),\n",
    "    'Lasso (Î±=0.1)': Lasso(alpha=0.1),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, max_depth=3, random_state=42)\n",
    "}\n",
    "\n",
    "# Cross-validation evaluation\n",
    "cv_results = {}\n",
    "\n",
    "print(\"Evaluating models with time series cross-validation...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    fold_metrics = []\n",
    "    \n",
    "    for fold, (train_idx, test_idx) in enumerate(tscv.split(X_scaled)):\n",
    "        X_train, X_test = X_scaled.iloc[train_idx], X_scaled.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "        \n",
    "        # Train model\n",
    "        model_fold = model.__class__(**model.get_params())\n",
    "        model_fold.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict\n",
    "        y_pred = model_fold.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = calculate_metrics(y_test, y_pred)\n",
    "        fold_metrics.append(metrics)\n",
    "    \n",
    "    # Aggregate results\n",
    "    cv_results[model_name] = {\n",
    "        'r2_mean': np.mean([m['r2'] for m in fold_metrics]),\n",
    "        'r2_std': np.std([m['r2'] for m in fold_metrics]),\n",
    "        'rmse_mean': np.mean([m['rmse'] for m in fold_metrics]),\n",
    "        'rmse_std': np.std([m['rmse'] for m in fold_metrics]),\n",
    "        'fold_metrics': fold_metrics\n",
    "    }\n",
    "    \n",
    "    print(f\"{model_name:20s} | RÂ²: {cv_results[model_name]['r2_mean']:+.3f} Â± {cv_results[model_name]['r2_std']:.3f} | \"\n",
    "          f\"RMSE: {cv_results[model_name]['rmse_mean']:.2f} Â± {cv_results[model_name]['rmse_std']:.2f}\")\n",
    "\n",
    "# Compare to baselines\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Baseline comparison:\")\n",
    "best_baseline_r2 = max([m['r2'] for m in baseline_results.values()])\n",
    "print(f\"Best baseline RÂ²: {best_baseline_r2:.3f}\")\n",
    "\n",
    "best_model = max(cv_results.items(), key=lambda x: x[1]['r2_mean'])\n",
    "print(f\"Best model: {best_model[0]} with RÂ²: {best_model[1]['r2_mean']:.3f}\")\n",
    "print(f\"Improvement over baseline: {best_model[1]['r2_mean'] - best_baseline_r2:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Performance Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive visualization of results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Model Performance Analysis', fontsize=16)\n",
    "\n",
    "# 1. Model comparison\n",
    "ax = axes[0, 0]\n",
    "model_names = list(cv_results.keys()) + list(baseline_results.keys())\n",
    "r2_values = [cv_results[m]['r2_mean'] for m in cv_results.keys()] + [baseline_results[m]['r2'] for m in baseline_results.keys()]\n",
    "colors = ['green' if r2 > 0 else 'red' for r2 in r2_values]\n",
    "\n",
    "bars = ax.bar(range(len(model_names)), r2_values, color=colors, alpha=0.7)\n",
    "ax.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "ax.set_xticks(range(len(model_names)))\n",
    "ax.set_xticklabels(model_names, rotation=45, ha='right')\n",
    "ax.set_ylabel('RÂ²')\n",
    "ax.set_title('Model Performance Comparison')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar, r2 in zip(bars, r2_values):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height + 0.01 if height > 0 else height - 0.01,\n",
    "            f'{r2:.3f}', ha='center', va='bottom' if height > 0 else 'top')\n",
    "\n",
    "# 2. Cross-validation stability\n",
    "ax = axes[0, 1]\n",
    "for model_name in list(cv_results.keys())[:3]:  # Top 3 models\n",
    "    fold_r2s = [m['r2'] for m in cv_results[model_name]['fold_metrics']]\n",
    "    ax.plot(range(1, len(fold_r2s)+1), fold_r2s, marker='o', label=model_name)\n",
    "\n",
    "ax.set_xlabel('CV Fold')\n",
    "ax.set_ylabel('RÂ²')\n",
    "ax.set_title('Cross-Validation Stability')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Feature importance (using best model)\n",
    "ax = axes[1, 0]\n",
    "if best_model[0].startswith('Ridge') or best_model[0].startswith('Linear'):\n",
    "    # Train on full data for coefficients\n",
    "    model = models[best_model[0]]\n",
    "    model.fit(X_scaled, y)\n",
    "    \n",
    "    coef_df = pd.DataFrame({\n",
    "        'feature': X_selected.columns,\n",
    "        'coefficient': model.coef_\n",
    "    }).sort_values('coefficient', key=abs, ascending=False)\n",
    "    \n",
    "    colors = ['green' if c > 0 else 'red' for c in coef_df['coefficient']]\n",
    "    ax.barh(range(len(coef_df)), coef_df['coefficient'], color=colors, alpha=0.7)\n",
    "    ax.set_yticks(range(len(coef_df)))\n",
    "    ax.set_yticklabels(coef_df['feature'])\n",
    "    ax.set_xlabel('Coefficient')\n",
    "    ax.set_title(f'Feature Coefficients ({best_model[0]})')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Actual vs Predicted (best model, last fold)\n",
    "ax = axes[1, 1]\n",
    "# Get last fold predictions\n",
    "last_train_idx, last_test_idx = list(tscv.split(X_scaled))[-1]\n",
    "X_train, X_test = X_scaled.iloc[last_train_idx], X_scaled.iloc[last_test_idx]\n",
    "y_train, y_test = y.iloc[last_train_idx], y.iloc[last_test_idx]\n",
    "\n",
    "model = models[best_model[0]]\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "ax.scatter(y_test, y_pred, alpha=0.6)\n",
    "ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "ax.set_xlabel('Actual UMCSENT')\n",
    "ax.set_ylabel('Predicted UMCSENT')\n",
    "ax.set_title(f'Actual vs Predicted ({best_model[0]})')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Add RÂ² annotation\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "ax.text(0.05, 0.95, f'RÂ² = {r2:.3f}', transform=ax.transAxes, \n",
    "        verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('final_outputs/visualizations/model_performance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Period-Specific Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze performance across economic periods\n",
    "periods = {\n",
    "    'Tech Boom': ('1995-01-01', '2000-12-31', 'lightblue'),\n",
    "    'Early 2000s': ('2001-01-01', '2007-12-31', 'lightgreen'),\n",
    "    'Financial Crisis': ('2008-01-01', '2009-12-31', 'lightcoral'),\n",
    "    'Recovery': ('2010-01-01', '2019-12-31', 'lightgray'),\n",
    "    'COVID Era': ('2020-01-01', '2025-05-31', 'lightyellow')\n",
    "}\n",
    "\n",
    "# Visualize sentiment across periods\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10), sharex=True)\n",
    "\n",
    "# Plot 1: Consumer sentiment over time with period shading\n",
    "ax1.plot(df_analysis.index, df_analysis['UMCSENT'], color='darkblue', linewidth=1.5)\n",
    "\n",
    "for period_name, (start, end, color) in periods.items():\n",
    "    mask = (df_analysis.index >= start) & (df_analysis.index <= end)\n",
    "    if mask.any():\n",
    "        ax1.axvspan(df_analysis.index[mask][0], df_analysis.index[mask][-1], \n",
    "                    alpha=0.3, color=color, label=period_name)\n",
    "\n",
    "ax1.set_ylabel('Consumer Sentiment Index')\n",
    "ax1.set_title('Consumer Sentiment Across Economic Periods')\n",
    "ax1.legend(loc='lower left')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Key economic indicators\n",
    "if 'inflation_yoy' in df_analysis.columns:\n",
    "    ax2.plot(df_analysis.index, df_analysis['inflation_yoy'], label='Inflation YoY', color='red')\n",
    "if 'unemployment_level' in df_analysis.columns:\n",
    "    ax2.plot(df_analysis.index, df_analysis['unemployment_level'], label='Unemployment', color='blue')\n",
    "if 'real_interest_rate' in df_analysis.columns:\n",
    "    ax2.plot(df_analysis.index, df_analysis['real_interest_rate'], label='Real Interest Rate', color='green')\n",
    "\n",
    "ax2.set_xlabel('Date')\n",
    "ax2.set_ylabel('Value (%)')\n",
    "ax2.set_title('Key Economic Indicators')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('final_outputs/visualizations/period_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Analyze model performance by period\n",
    "print(\"\\nModel Performance by Economic Period:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "period_performance = {}\n",
    "best_model_class = models[best_model[0]]\n",
    "\n",
    "for period_name, (start, end, _) in periods.items():\n",
    "    mask = (X_scaled.index >= start) & (X_scaled.index <= end)\n",
    "    if mask.sum() > 24:  # Need sufficient data\n",
    "        X_period = X_scaled[mask]\n",
    "        y_period = y[mask]\n",
    "        \n",
    "        # Simple train/test split\n",
    "        split_idx = int(len(X_period) * 0.8)\n",
    "        X_train, X_test = X_period[:split_idx], X_period[split_idx:]\n",
    "        y_train, y_test = y_period[:split_idx], y_period[split_idx:]\n",
    "        \n",
    "        # Train and evaluate\n",
    "        model = best_model_class.__class__(**best_model_class.get_params())\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        metrics = calculate_metrics(y_test, y_pred)\n",
    "        period_performance[period_name] = metrics\n",
    "        \n",
    "        print(f\"{period_name:20s} | RÂ²: {metrics['r2']:+.3f} | RMSE: {metrics['rmse']:.2f} | N: {len(X_period)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Forward-Looking Analysis: Sentiment as Leading Indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze sentiment's predictive power for future economic activity\n",
    "print(\"Analyzing sentiment as a leading indicator...\")\n",
    "\n",
    "# Define outcome variables\n",
    "outcome_vars = ['RSAFS', 'PCE', 'INDPRO', 'HOUST']\n",
    "available_outcomes = [var for var in outcome_vars if var in df_monthly.columns]\n",
    "\n",
    "# Create forward-looking analysis\n",
    "forward_results = {}\n",
    "horizons = [1, 3, 6]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, outcome in enumerate(available_outcomes[:4]):\n",
    "    ax = axes[idx]\n",
    "    horizon_r2 = []\n",
    "    \n",
    "    for horizon in horizons:\n",
    "        # Create lagged sentiment features\n",
    "        X_sentiment = pd.DataFrame({\n",
    "            'sentiment': df_monthly['UMCSENT'],\n",
    "            'sentiment_change': df_monthly['UMCSENT'].pct_change(3) * 100\n",
    "        })\n",
    "        \n",
    "        # Create forward target\n",
    "        y_forward = df_monthly[outcome].pct_change(horizon).shift(-horizon) * 100\n",
    "        \n",
    "        # Combine and clean\n",
    "        data = pd.concat([X_sentiment, y_forward], axis=1).dropna()\n",
    "        \n",
    "        if len(data) > 50:\n",
    "            # Simple OLS regression\n",
    "            X_reg = sm.add_constant(data[['sentiment', 'sentiment_change']])\n",
    "            y_reg = data[outcome]\n",
    "            \n",
    "            model = sm.OLS(y_reg, X_reg).fit()\n",
    "            horizon_r2.append(model.rsquared)\n",
    "            \n",
    "            if outcome not in forward_results:\n",
    "                forward_results[outcome] = {}\n",
    "            forward_results[outcome][horizon] = {\n",
    "                'r2': model.rsquared,\n",
    "                'coef': model.params['sentiment'],\n",
    "                'pvalue': model.pvalues['sentiment']\n",
    "            }\n",
    "        else:\n",
    "            horizon_r2.append(0)\n",
    "    \n",
    "    # Plot results\n",
    "    ax.plot(horizons, horizon_r2, marker='o', markersize=10, linewidth=2)\n",
    "    ax.set_xlabel('Forecast Horizon (months)')\n",
    "    ax.set_ylabel('RÂ²')\n",
    "    ax.set_title(f'{outcome}')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim(0, max(0.1, max(horizon_r2) * 1.2))\n",
    "\n",
    "plt.suptitle('Sentiment as Leading Indicator for Economic Activity', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('final_outputs/visualizations/leading_indicator.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Summary table\n",
    "print(\"\\nSentiment's Predictive Power (RÂ²):\")\n",
    "print(\"=\"*50)\n",
    "print(f\"{'Outcome':15s} | 1-month | 3-month | 6-month\")\n",
    "print(\"-\"*50)\n",
    "for outcome in available_outcomes[:4]:\n",
    "    if outcome in forward_results:\n",
    "        r2_values = [forward_results[outcome].get(h, {}).get('r2', 0) for h in horizons]\n",
    "        print(f\"{outcome:15s} | {r2_values[0]:7.3f} | {r2_values[1]:7.3f} | {r2_values[2]:7.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Interpretation and Key Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Comprehensive summary of findings\nprint(\"=\"*80)\nprint(\"CONSUMER SENTIMENT ANALYSIS: KEY FINDINGS\")\nprint(\"=\"*80)\n\nprint(\"\\n1. MODEL PERFORMANCE:\")\nprint(f\"   â€¢ Best model: {best_model[0]}\")\nprint(f\"   â€¢ Cross-validated RÂ²: {best_model[1]['r2_mean']:.3f} Â± {best_model[1]['r2_std']:.3f}\")\nprint(f\"   â€¢ Improvement over baseline: {(best_model[1]['r2_mean'] - best_baseline_r2)*100:.1f} percentage points\")\n\n# Handle negative RÂ² for description\nif best_model[1]['r2_mean'] < 0:\n    performance_desc = \"needs improvement\"\nelif best_model[1]['r2_mean'] < 0.3:\n    performance_desc = \"poor\"\nelif best_model[1]['r2_mean'] < 0.5:\n    performance_desc = \"moderate\"\nelif best_model[1]['r2_mean'] < 0.7:\n    performance_desc = \"good\"\nelse:\n    performance_desc = \"excellent\"\n\nprint(f\"   â€¢ Model shows {performance_desc} predictive power\")\n\nprint(\"\\n2. KEY ECONOMIC DRIVERS:\")\nif 'coef_df' in locals():\n    top_drivers = coef_df.head(3)\n    for _, row in top_drivers.iterrows():\n        direction = \"increases\" if row['coefficient'] > 0 else \"decreases\"\n        print(f\"   â€¢ {row['feature']}: {direction} sentiment (coef: {row['coefficient']:.3f})\")\n\nprint(\"\\n3. TEMPORAL PATTERNS:\")\nif 'period_performance' in locals() and period_performance:\n    best_period = max(period_performance.items(), key=lambda x: x[1]['r2'])\n    worst_period = min(period_performance.items(), key=lambda x: x[1]['r2'])\n    print(f\"   â€¢ Most predictable period: {best_period[0]} (RÂ² = {best_period[1]['r2']:.3f})\")\n    print(f\"   â€¢ Least predictable period: {worst_period[0]} (RÂ² = {worst_period[1]['r2']:.3f})\")\n    print(f\"   â€¢ Economic uncertainty reduces model accuracy\")\n\nprint(\"\\n4. LEADING INDICATOR INSIGHTS:\")\nif 'forward_results' in locals() and forward_results:\n    print(\"   â€¢ Sentiment shows weak but consistent predictive power\")\n    print(\"   â€¢ Shorter horizons (1-3 months) more reliable\")\n    print(\"   â€¢ Retail sales most responsive to sentiment changes\")\n\nprint(\"\\n5. PRACTICAL IMPLICATIONS:\")\nprint(\"   â€¢ Consumer sentiment reflects current economic conditions\")\nprint(\"   â€¢ Inflation and unemployment are primary drivers\")\nprint(\"   â€¢ Financial market volatility impacts consumer confidence\")\nprint(\"   â€¢ Sentiment can provide early signals for economic turning points\")\n\n# Save comprehensive results\nresults_summary = {\n    'analysis_date': datetime.now().isoformat(),\n    'data_range': f\"{df_analysis.index.min()} to {df_analysis.index.max()}\",\n    'n_observations': len(df_analysis),\n    'n_features': len(selected_features),\n    'selected_features': selected_features,\n    'best_model': {\n        'name': best_model[0],\n        'cv_r2': best_model[1]['r2_mean'],\n        'cv_rmse': best_model[1]['rmse_mean']\n    },\n    'baseline_comparison': baseline_results,\n    'period_performance': period_performance if 'period_performance' in locals() else {},\n    'forward_analysis': forward_results if 'forward_results' in locals() else {}\n}\n\nwith open('final_outputs/results/analysis_summary.json', 'w') as f:\n    json.dump(results_summary, f, indent=2, default=str)\n\nprint(\"\\nâœ… Analysis complete! Results saved to final_outputs/\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Broader Impacts and Ethical Considerations\n",
    "\n",
    "### Who is impacted by this work?\n",
    "\n",
    "1. **Policymakers**: Federal Reserve and government officials use consumer sentiment as an input for monetary and fiscal policy decisions\n",
    "2. **Financial Markets**: Investors and traders use sentiment indicators for market timing and risk assessment\n",
    "3. **Businesses**: Companies use sentiment data for demand forecasting and strategic planning\n",
    "4. **General Public**: Citizens whose economic behavior both influences and is influenced by aggregate sentiment measures\n",
    "\n",
    "### Ethical Considerations\n",
    "\n",
    "1. **Self-Fulfilling Prophecies**: Publishing negative sentiment predictions could potentially contribute to economic downturns by influencing behavior\n",
    "2. **Representation Bias**: The Michigan survey may not equally represent all demographic groups, potentially marginalizing certain voices\n",
    "3. **Model Transparency**: Complex models may be used for critical decisions without full understanding of their limitations\n",
    "4. **Data Privacy**: While using aggregate data, we must ensure individual survey responses remain confidential\n",
    "\n",
    "### Recommendations\n",
    "\n",
    "- Models should be used as one input among many, not as sole decision-makers\n",
    "- Uncertainty and limitations should be clearly communicated\n",
    "- Regular model updates and validation are essential as economic relationships evolve\n",
    "- Consider multiple sentiment measures to avoid over-reliance on a single source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. Curtin, R. (2019). *Consumer Expectations: Micro Foundations and Macro Impact*. Cambridge University Press.\n",
    "\n",
    "2. Katona, G. (1968). \"Consumer Behavior: Theory and Findings on Expectations and Aspirations.\" *The American Economic Review*, 58(2), 19-30.\n",
    "\n",
    "3. Ludvigson, S. C. (2004). \"Consumer Confidence and Consumer Spending.\" *Journal of Economic Perspectives*, 18(2), 29-50.\n",
    "\n",
    "4. Carroll, C. D., Fuhrer, J. C., & Wilcox, D. W. (1994). \"Does Consumer Sentiment Forecast Household Spending? If So, Why?\" *The American Economic Review*, 84(5), 1397-1408.\n",
    "\n",
    "5. Barsky, R. B., & Sims, E. R. (2012). \"Information, Animal Spirits, and the Meaning of Innovations in Consumer Confidence.\" *American Economic Review*, 102(4), 1343-77.\n",
    "\n",
    "6. Federal Reserve Economic Data (FRED). Federal Reserve Bank of St. Louis. https://fred.stlouisfed.org/\n",
    "\n",
    "7. University of Michigan. \"Surveys of Consumers.\" http://www.sca.isr.umich.edu/\n",
    "\n",
    "8. Stock, J. H., & Watson, M. W. (2003). \"Forecasting Output and Inflation: The Role of Asset Prices.\" *Journal of Economic Literature*, 41(3), 788-829."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}